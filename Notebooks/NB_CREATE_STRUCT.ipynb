{"cells":[{"cell_type":"code","source":["from pyspark.sql import SparkSession\n","import pyspark.sql.functions as F\n","from pyspark.sql.types import *\n","from notebookutils import mssparkutils\n","import pandas as pd\n","\n","print(f\"Versión de Spark: {spark.version}\")"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":4,"statement_ids":[4],"state":"finished","livy_statement_state":"available","session_id":"d9d065e3-161e-424b-9833-db7fda3a5810","normalized_state":"finished","queued_time":"2025-07-03T12:02:52.1428718Z","session_start_time":null,"execution_start_time":"2025-07-03T12:02:52.1440259Z","execution_finish_time":"2025-07-03T12:02:52.4177477Z","parent_msg_id":"5b1f8aa8-bdc1-4d1c-b8b0-f2fb0955803c"},"text/plain":"StatementMeta(, d9d065e3-161e-424b-9833-db7fda3a5810, 4, Finished, Available, Finished)"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Versión de Spark: 3.5.1.5.4.20250519.1\n"]}],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"23bcd173-52fd-4ac5-afed-ab43df48f4b3"},{"cell_type":"code","source":["spark ="],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"485c15cb-aba6-4e80-9b5c-c9533b91b9eb"},{"cell_type":"code","source":["# Crear estructura de esquemas en el lakehouse para almacenar nuestras tablas procesadas\n","spark = SparkSession.builder.getOrCreate()\n","try:\n","    #creando carpetas para datos procesados, esquema medallion. Premisa: Capa Bronze existente, Pipeline Copy Data\n","    spark.sql(\"CREATE SCHEMA IF NOT EXISTS bronze\")\n","    spark.sql(\"CREATE SCHEMA IF NOT EXISTS silver\")\n","    spark.sql(\"CREATE SCHEMA IF NOT EXISTS gold\")\n","    print(\"Esquemas creados\")\n","except Exception as e:\n","    print(f\"Error al crear esquemas: {str(e)}\")"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":25,"statement_ids":[25],"state":"finished","livy_statement_state":"available","session_id":"c8b8caf9-7d05-45e6-9068-9d835cc11ee9","normalized_state":"finished","queued_time":"2025-07-03T03:35:16.46338Z","session_start_time":null,"execution_start_time":"2025-07-03T03:35:16.4645067Z","execution_finish_time":"2025-07-03T03:35:17.2656395Z","parent_msg_id":"0cb4a428-993e-4013-bfb7-71585a8183b0"},"text/plain":"StatementMeta(, c8b8caf9-7d05-45e6-9068-9d835cc11ee9, 25, Finished, Available, Finished)"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Error al crear esquemas: An error occurred while calling o348.sql.\n: java.lang.RuntimeException: java.lang.reflect.InvocationTargetException\n\tat com.microsoft.azure.trident.spark.TridentCoreProxy.failCreateDbIfTrident(TridentCoreProxy.java:300)\n\tat org.apache.spark.sql.catalyst.catalog.SessionCatalog.createDatabase(SessionCatalog.scala:317)\n\tat org.apache.spark.sql.execution.datasources.v2.V2SessionCatalog.createNamespace(V2SessionCatalog.scala:327)\n\tat org.apache.spark.sql.connector.catalog.DelegatingCatalogExtension.createNamespace(DelegatingCatalogExtension.java:163)\n\tat org.apache.spark.sql.execution.datasources.v2.CreateNamespaceExec.run(CreateNamespaceExec.scala:47)\n\tat org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result$lzycompute(V2CommandExec.scala:43)\n\tat org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result(V2CommandExec.scala:43)\n\tat org.apache.spark.sql.execution.datasources.v2.V2CommandExec.executeCollect(V2CommandExec.scala:49)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:214)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:132)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:220)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:101)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:961)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:68)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:214)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:202)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:461)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:461)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:36)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:36)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:36)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:437)\n\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:202)\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:186)\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:180)\n\tat org.apache.spark.sql.Dataset.<init>(Dataset.scala:231)\n\tat org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:101)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:961)\n\tat org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:98)\n\tat org.apache.spark.sql.SparkSession.$anonfun$sql$1(SparkSession.scala:699)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:961)\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:690)\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:720)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\nCaused by: java.lang.reflect.InvocationTargetException\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n\tat com.microsoft.azure.trident.spark.TridentCoreProxy.failCreateDbIfTrident(TridentCoreProxy.java:297)\n\t... 46 more\nCaused by: java.lang.RuntimeException: Feature not supported on Apache Spark in Microsoft Fabric. Provided context: { spark.trident.pbiHost=api.fabric.microsoft.com, fs.defaultFS=abfss://a25d3567-f874-4d11-b9d2-fe489e7b19ca@onelake.dfs.fabric.microsoft.com/, spark.openlineage.transport.sparkcore_ingestion_enable=false, trident.capacity.id=354be817-e4e3-4784-a7b5-c5dc312aa29b, spark.fabric.environmentDetails={}, trident.operation.type=SessionCreation, trident.workspace.id=a25d3567-f874-4d11-b9d2-fe489e7b19ca, trident.tokenservice.zkcache.enabled=*****, trident.catalog.metastore.workspaceId=a25d3567-f874-4d11-b9d2-fe489e7b19ca, spark.fabric.pools.category=Starter, spark.fabric.pools.poolHitEventTime=2025-07-03T02:46:56.8189963Z, trident.artifact.workspace.id=a25d3567-f874-4d11-b9d2-fe489e7b19ca, trident.activity.id=c8b8caf9-7d05-45e6-9068-9d835cc11ee9, spark.trident.lineage.enabled=false, trident.artifact.type=SynapseNotebook, trident.lineage.enabled=False, trident.materializedview.libraries.enabled=false, trident.lakehouse.tokenservice.endpoint=htt***ken, spark.trident.disable_autolog=false, fs.homeDir=/b64205f3-e3bf-4a25-b091-d0aff87399ee, spark.fabric.pool.name=Starter Pool, spark.synapse.nbs.session.timeout=1200000, trident.tenant.id=f1fd1c08-d9b7-4c57-a4cf-21fd8a4260bb, trident.esri.libraries.enabled=false, trident.catalog.metastore.lakehouseName=LH_BRICK_VISTA_MC, spark.fabric.resourceProfile=writeHeavy, spark.trident.autotune.fetchSAS.url=htt***Url, trident.moniker.id=c8b8caf9-7d05-45e6-9068-9d835cc11ee9, spark.trident.highconcurrency.enabled=false, spark.trident.session.submittedAt=1751510816881, trident.artifact.id=883efbcb-cf68-4297-8b76-4d7e963a0a7b, spark.cluster.type=*****, spark.synapse.context.notebookname=NB_CREATE_STRUCT, spark.fabric.pools.vhdOverride=false, spark.fabric.pools.poolHit=true, trident.lakehouse.name=LH_BRICK_VISTA_MC, trident.lakehouse.id=b64205f3-e3bf-4a25-b091-d0aff87399ee, spark.synapse.nbs.kernelid=6da17e45-7598-4bac-a3b7-6bb0a4fc97d7, trident.session.token=eyJ***tsg }\n\tat com.microsoft.azure.trident.core.TridentHelper.failIfValidTridentContext(TridentHelper.java:271)\n\t... 51 more\n\n"]}],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"1d3e5d2b-16bd-4e79-9b67-1e16ec5a84b0"},{"cell_type":"code","source":["from pyspark.sql import SparkSession\n","import pyspark.sql.functions as F\n","from pyspark.sql.types import *\n","from notebookutils import mssparkutils as mu\n","import pandas as pd"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":20,"statement_ids":[20],"state":"finished","livy_statement_state":"available","session_id":"c8b8caf9-7d05-45e6-9068-9d835cc11ee9","normalized_state":"finished","queued_time":"2025-07-03T03:18:36.9716846Z","session_start_time":null,"execution_start_time":"2025-07-03T03:18:36.9727326Z","execution_finish_time":"2025-07-03T03:18:37.2606232Z","parent_msg_id":"ce7300dd-b592-4851-9f9f-34b28851336d"},"text/plain":"StatementMeta(, c8b8caf9-7d05-45e6-9068-9d835cc11ee9, 20, Finished, Available, Finished)"},"metadata":{}}],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"84e78c5b-ab14-410c-b044-8a034d0a09e3"},{"cell_type":"code","source":[],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"0583a9cd-8a0d-46d8-ae9e-fddd9d5a6f76"},{"cell_type":"code","source":["from pyspark.sql import SparkSession\n","\n","spark = SparkSession.builder.getOrCreate()\n","\n","# Ruta raíz del Lakehouse por defecto en Fabric\n","lakehouse_base_path = \"/lakehouse/default\"\n","\n","# Nombres de esquemas/capas\n","layers = [\"bronze\", \"silver\", \"gold\"]\n","\n","try:\n","    for layer in layers:\n","        # Crear base de datos en Spark (esquema lógico)\n","        spark.sql(f\"CREATE DATABASE IF NOT EXISTS {layer}\")\n","        \n","        # Crear carpeta física en el Lakehouse si no existe\n","        path = f\"{lakehouse_base_path}/{layer}\"\n","        dbutils.fs.mkdirs(path)\n","        \n","        print(f\"Esquema y carpeta '{layer}' creados o ya existentes.\")\n","except Exception as e:\n","    print(f\"❌ Error: {str(e)}\")"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":22,"statement_ids":[22],"state":"finished","livy_statement_state":"available","session_id":"c8b8caf9-7d05-45e6-9068-9d835cc11ee9","normalized_state":"finished","queued_time":"2025-07-03T03:25:22.7718359Z","session_start_time":null,"execution_start_time":"2025-07-03T03:25:22.7728774Z","execution_finish_time":"2025-07-03T03:25:23.5866027Z","parent_msg_id":"3eaf16f7-633e-40c6-8a9b-6f84342d81a5"},"text/plain":"StatementMeta(, c8b8caf9-7d05-45e6-9068-9d835cc11ee9, 22, Finished, Available, Finished)"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["❌ Error: An error occurred while calling o348.sql.\n: java.lang.RuntimeException: java.lang.reflect.InvocationTargetException\n\tat com.microsoft.azure.trident.spark.TridentCoreProxy.failCreateDbIfTrident(TridentCoreProxy.java:300)\n\tat org.apache.spark.sql.catalyst.catalog.SessionCatalog.createDatabase(SessionCatalog.scala:317)\n\tat org.apache.spark.sql.execution.datasources.v2.V2SessionCatalog.createNamespace(V2SessionCatalog.scala:327)\n\tat org.apache.spark.sql.connector.catalog.DelegatingCatalogExtension.createNamespace(DelegatingCatalogExtension.java:163)\n\tat org.apache.spark.sql.execution.datasources.v2.CreateNamespaceExec.run(CreateNamespaceExec.scala:47)\n\tat org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result$lzycompute(V2CommandExec.scala:43)\n\tat org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result(V2CommandExec.scala:43)\n\tat org.apache.spark.sql.execution.datasources.v2.V2CommandExec.executeCollect(V2CommandExec.scala:49)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:214)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:132)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:220)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:101)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:961)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:68)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:214)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:202)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:461)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:461)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:36)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:36)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:36)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:437)\n\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:202)\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:186)\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:180)\n\tat org.apache.spark.sql.Dataset.<init>(Dataset.scala:231)\n\tat org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:101)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:961)\n\tat org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:98)\n\tat org.apache.spark.sql.SparkSession.$anonfun$sql$1(SparkSession.scala:699)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:961)\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:690)\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:720)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\nCaused by: java.lang.reflect.InvocationTargetException\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n\tat com.microsoft.azure.trident.spark.TridentCoreProxy.failCreateDbIfTrident(TridentCoreProxy.java:297)\n\t... 46 more\nCaused by: java.lang.RuntimeException: Feature not supported on Apache Spark in Microsoft Fabric. Provided context: { spark.trident.pbiHost=api.fabric.microsoft.com, fs.defaultFS=abfss://a25d3567-f874-4d11-b9d2-fe489e7b19ca@onelake.dfs.fabric.microsoft.com/, spark.openlineage.transport.sparkcore_ingestion_enable=false, trident.capacity.id=354be817-e4e3-4784-a7b5-c5dc312aa29b, spark.fabric.environmentDetails={}, trident.operation.type=SessionCreation, trident.workspace.id=a25d3567-f874-4d11-b9d2-fe489e7b19ca, trident.tokenservice.zkcache.enabled=*****, trident.catalog.metastore.workspaceId=a25d3567-f874-4d11-b9d2-fe489e7b19ca, spark.fabric.pools.category=Starter, spark.fabric.pools.poolHitEventTime=2025-07-03T02:46:56.8189963Z, trident.artifact.workspace.id=a25d3567-f874-4d11-b9d2-fe489e7b19ca, trident.activity.id=c8b8caf9-7d05-45e6-9068-9d835cc11ee9, spark.trident.lineage.enabled=false, trident.artifact.type=SynapseNotebook, trident.lineage.enabled=False, trident.materializedview.libraries.enabled=false, trident.lakehouse.tokenservice.endpoint=htt***ken, spark.trident.disable_autolog=false, fs.homeDir=/b64205f3-e3bf-4a25-b091-d0aff87399ee, spark.fabric.pool.name=Starter Pool, spark.synapse.nbs.session.timeout=1200000, trident.tenant.id=f1fd1c08-d9b7-4c57-a4cf-21fd8a4260bb, trident.esri.libraries.enabled=false, trident.catalog.metastore.lakehouseName=LH_BRICK_VISTA_MC, spark.fabric.resourceProfile=writeHeavy, spark.trident.autotune.fetchSAS.url=htt***Url, trident.moniker.id=c8b8caf9-7d05-45e6-9068-9d835cc11ee9, spark.trident.highconcurrency.enabled=false, spark.trident.session.submittedAt=1751510816881, trident.artifact.id=883efbcb-cf68-4297-8b76-4d7e963a0a7b, spark.cluster.type=*****, spark.synapse.context.notebookname=NB_CREATE_STRUCT, spark.fabric.pools.vhdOverride=false, spark.fabric.pools.poolHit=true, trident.lakehouse.name=LH_BRICK_VISTA_MC, trident.lakehouse.id=b64205f3-e3bf-4a25-b091-d0aff87399ee, spark.synapse.nbs.kernelid=6da17e45-7598-4bac-a3b7-6bb0a4fc97d7, trident.session.token=eyJ***tsg }\n\tat com.microsoft.azure.trident.core.TridentHelper.failIfValidTridentContext(TridentHelper.java:271)\n\t... 51 more\n\n"]}],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"9b9f2352-fb76-492e-9337-8443746066b0"},{"cell_type":"code","source":["try:\n","    mssparkutils.fs.mkdirs(\"Files/bronze\")\n","    mssparkutils.fs.mkdirs(\"Files/silver\")\n","    mssparkutils.fs.mkdirs(\"Files/gold\")\n","    mssparkutils.fs.mkdirs(\"Files/processed\")\n","    print(\"✅ Estructura de carpetas creada correctamente\")\n","except Exception as e:\n","    print(f\"Error al crear carpetas: {str(e)}\")\n","    \n"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":8,"statement_ids":[8],"state":"finished","livy_statement_state":"available","session_id":"c8b8caf9-7d05-45e6-9068-9d835cc11ee9","normalized_state":"finished","queued_time":"2025-07-03T03:08:29.2172693Z","session_start_time":null,"execution_start_time":"2025-07-03T03:08:29.2183382Z","execution_finish_time":"2025-07-03T03:08:30.0034418Z","parent_msg_id":"3567d958-eea5-41e6-9d55-181fe5a63f0c"},"text/plain":"StatementMeta(, c8b8caf9-7d05-45e6-9068-9d835cc11ee9, 8, Finished, Available, Finished)"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["✅ Estructura de carpetas creada correctamente\n"]}],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"cdfba7ca-8aa0-4d95-9421-0f18877763f8"},{"cell_type":"code","source":["schema_brokers = StructType([\n","    StructField(\"BrokerID\",IntegerType(),False),\n","    StructField(\"BrokerName\",StringType(),False),\n","    StructField(\"Region\",StringType(),False),\n","    StructField(\"Email\",StringType(),False)\n","\n","])"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":9,"statement_ids":[9],"state":"finished","livy_statement_state":"available","session_id":"c8b8caf9-7d05-45e6-9068-9d835cc11ee9","normalized_state":"finished","queued_time":"2025-07-03T03:08:31.3154592Z","session_start_time":null,"execution_start_time":"2025-07-03T03:08:31.3165279Z","execution_finish_time":"2025-07-03T03:08:31.6157199Z","parent_msg_id":"663860ef-b065-42aa-83ff-1a89fe40868b"},"text/plain":"StatementMeta(, c8b8caf9-7d05-45e6-9068-9d835cc11ee9, 9, Finished, Available, Finished)"},"metadata":{}}],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"b288464e-9139-4389-a179-f07422b0f8d6"},{"cell_type":"code","source":["try:\n","    df_brokers =  spark.read.format(\"csv\") \\\n","    .option(\"header\", \"true\") \\\n","    .schema(schema_brokers) \\\n","    .load(\"Files/raw/brokers.csv\")\n","\n","except Exception as e:\n","    print(f\"Error al leer archivos: {str(e)}\")"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":10,"statement_ids":[10],"state":"finished","livy_statement_state":"available","session_id":"c8b8caf9-7d05-45e6-9068-9d835cc11ee9","normalized_state":"finished","queued_time":"2025-07-03T03:08:34.308126Z","session_start_time":null,"execution_start_time":"2025-07-03T03:08:34.3092791Z","execution_finish_time":"2025-07-03T03:08:35.0904392Z","parent_msg_id":"3b22abc5-e533-460d-80eb-a927fc5fb21d"},"text/plain":"StatementMeta(, c8b8caf9-7d05-45e6-9068-9d835cc11ee9, 10, Finished, Available, Finished)"},"metadata":{}}],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"9d8fc55f-6d6c-4920-a808-596ffc0fbc0d"},{"cell_type":"code","source":["df_brokers_bronze = df_brokers.withColumn(\"_Load_date\",F.current_timestamp()) \\\n",".withColumn(\"_user\",F.lit(\"admin\"))"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":11,"statement_ids":[11],"state":"finished","livy_statement_state":"available","session_id":"c8b8caf9-7d05-45e6-9068-9d835cc11ee9","normalized_state":"finished","queued_time":"2025-07-03T03:08:37.3316325Z","session_start_time":null,"execution_start_time":"2025-07-03T03:08:37.3329102Z","execution_finish_time":"2025-07-03T03:08:37.5803787Z","parent_msg_id":"28f99702-a7c9-4191-b881-a292b0c05501"},"text/plain":"StatementMeta(, c8b8caf9-7d05-45e6-9068-9d835cc11ee9, 11, Finished, Available, Finished)"},"metadata":{}}],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"a9a64c27-d029-4a8f-af5b-ce56ab24b240"},{"cell_type":"code","source":["try:\n","    df_brokers_bronze.write.format(\"delta\").mode(\"overwrite\").save(\"Files/bronze/brokers_bronze\")\n","\n","except Exception as e:\n","    print(f\"Error al leer archivos: {str(e)}\")"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":12,"statement_ids":[12],"state":"finished","livy_statement_state":"available","session_id":"c8b8caf9-7d05-45e6-9068-9d835cc11ee9","normalized_state":"finished","queued_time":"2025-07-03T03:08:39.9208058Z","session_start_time":null,"execution_start_time":"2025-07-03T03:08:39.9218509Z","execution_finish_time":"2025-07-03T03:08:51.5765238Z","parent_msg_id":"6723c6aa-936c-4e0e-b333-428109281732"},"text/plain":"StatementMeta(, c8b8caf9-7d05-45e6-9068-9d835cc11ee9, 12, Finished, Available, Finished)"},"metadata":{}}],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"8cc9b259-670c-49e5-9608-2436965725f4"},{"cell_type":"code","source":["df_brokers_bronze.createOrReplaceTempView(\"bronze_brokers\")\n","df_brokers_bronze.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"bronze.brokers\")"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":26,"statement_ids":[26],"state":"finished","livy_statement_state":"available","session_id":"c8b8caf9-7d05-45e6-9068-9d835cc11ee9","normalized_state":"finished","queued_time":"2025-07-03T03:39:48.9492142Z","session_start_time":null,"execution_start_time":"2025-07-03T03:39:48.9504168Z","execution_finish_time":"2025-07-03T03:39:50.3906213Z","parent_msg_id":"e893317e-3a8e-4db8-9c28-42e737d880c8"},"text/plain":"StatementMeta(, c8b8caf9-7d05-45e6-9068-9d835cc11ee9, 26, Finished, Available, Finished)"},"metadata":{}},{"output_type":"error","ename":"AnalysisException","evalue":"[SCHEMA_NOT_FOUND] The schema `bronze` cannot be found. Verify the spelling and correctness of the schema and catalog.\nIf you did not qualify the name with a catalog, verify the current_schema() output, or qualify the name with the correct catalog.\nTo tolerate the error on drop use DROP SCHEMA IF EXISTS.","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)","Cell \u001b[0;32mIn[74], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m df_brokers_bronze\u001b[38;5;241m.\u001b[39mcreateOrReplaceTempView(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbronze_brokers\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 2\u001b[0m df_brokers_bronze\u001b[38;5;241m.\u001b[39mwrite\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdelta\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mmode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moverwrite\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39msaveAsTable(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbronze.brokers\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n","File \u001b[0;32m/opt/spark/python/lib/pyspark.zip/pyspark/sql/readwriter.py:1586\u001b[0m, in \u001b[0;36mDataFrameWriter.saveAsTable\u001b[0;34m(self, name, format, mode, partitionBy, **options)\u001b[0m\n\u001b[1;32m   1584\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mformat\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1585\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;28mformat\u001b[39m)\n\u001b[0;32m-> 1586\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jwrite\u001b[38;5;241m.\u001b[39msaveAsTable(name)\n","File \u001b[0;32m~/cluster-env/trident_env/lib/python3.11/site-packages/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m get_return_value(\n\u001b[1;32m   1323\u001b[0m     answer, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_id, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname)\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n","File \u001b[0;32m/opt/spark/python/lib/pyspark.zip/pyspark/errors/exceptions/captured.py:185\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    181\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[1;32m    182\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    183\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    184\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[0;32m--> 185\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    186\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    187\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n","\u001b[0;31mAnalysisException\u001b[0m: [SCHEMA_NOT_FOUND] The schema `bronze` cannot be found. Verify the spelling and correctness of the schema and catalog.\nIf you did not qualify the name with a catalog, verify the current_schema() output, or qualify the name with the correct catalog.\nTo tolerate the error on drop use DROP SCHEMA IF EXISTS."]}],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"aec0f816-13e8-4136-9dec-ddcaffe996ab"},{"cell_type":"code","source":[],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"2a75f653-41a0-4e8c-ac7f-a2b81eb73b21"}],"metadata":{"kernel_info":{"name":"synapse_pyspark"},"kernelspec":{"name":"synapse_pyspark","display_name":"synapse_pyspark"},"language_info":{"name":"python"},"microsoft":{"language":"python","language_group":"synapse_pyspark","ms_spell_check":{"ms_spell_check_language":"en"}},"nteract":{"version":"nteract-front-end@1.0.0"},"spark_compute":{"compute_id":"/trident/default","session_options":{"conf":{"spark.synapse.nbs.session.timeout":"1200000"}}},"dependencies":{"lakehouse":{"known_lakehouses":[{"id":"506aa2d7-a37c-4ddd-80b2-dfd687fe903a"}],"default_lakehouse":"506aa2d7-a37c-4ddd-80b2-dfd687fe903a","default_lakehouse_name":"LKH_BRICKVISTA_001_MC","default_lakehouse_workspace_id":"a25d3567-f874-4d11-b9d2-fe489e7b19ca"}}},"nbformat":4,"nbformat_minor":5}